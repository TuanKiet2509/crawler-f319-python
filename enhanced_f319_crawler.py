# -*- coding: utf-8 -*-
"""
Enhanced F319 Comment Crawler
S·ª≠ d·ª•ng search approach hi·ªáu qu·∫£ h∆°n
"""

import time
import csv
import sys
import logging
import re
import zipfile
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import config

class EnhancedF319Crawler:
    def __init__(self):
        self.driver = None
        self.comments_data = []
        self.search_base_url = None
        self.user_id = None
        self.username = None
        self.setup_logging()
        
    def setup_logging(self):
        """Thi·∫øt l·∫≠p logging"""
        if config.ENABLE_LOGGING:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler('enhanced_crawler.log', encoding='utf-8'),
                    logging.StreamHandler(sys.stdout)
                ]
            )
        self.logger = logging.getLogger(__name__)
        
    def setup_driver(self):
        """Kh·ªüi t·∫°o Chrome driver"""
        try:
            self.logger.info("üöÄ ƒêang kh·ªüi t·∫°o Chrome driver...")
            chrome_options = Options()
            
            if config.HEADLESS_MODE:
                chrome_options.add_argument("--headless")
            
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            # FIX: Th√™m fallback options khi webdriver-manager th·∫•t b·∫°i
            driver_path = None
            
            try:
                # Th·ª≠ webdriver-manager tr∆∞·ªõc
                self.logger.info("üîß Th·ª≠ s·ª≠ d·ª•ng webdriver-manager...")
                wdm_path = ChromeDriverManager().install()
                
                # FIX: Ki·ªÉm tra xem WebDriver Manager c√≥ tr·∫£ v·ªÅ ƒë√∫ng file kh√¥ng
                import os
                from pathlib import Path
                
                # Ki·ªÉm tra xem file c√≥ ph·∫£i l√† chromedriver th·∫≠t kh√¥ng
                # Kh√¥ng ch·ªâ ki·ªÉm tra t√™n file m√† c√≤n ki·ªÉm tra c√≥ th·ªÉ th·ª±c thi
                is_valid_chromedriver = False
                
                # Ki·ªÉm tra n·∫øu file c√≥ t√™n ch√≠nh x√°c l√† 'chromedriver' (kh√¥ng c√≥ extension)
                if Path(wdm_path).name == 'chromedriver':
                    if os.path.exists(wdm_path) and os.access(wdm_path, os.X_OK):
                        driver_path = wdm_path
                        is_valid_chromedriver = True
                        self.logger.info(f"‚úÖ S·ª≠ d·ª•ng ChromeDriver t·ª´ webdriver-manager: {driver_path}")
                
                # N·∫øu WebDriver Manager tr·∫£ v·ªÅ file sai
                if not is_valid_chromedriver:
                    self.logger.warning(f"‚ö†Ô∏è WebDriver Manager tr·∫£ v·ªÅ file kh√¥ng h·ª£p l·ªá: {wdm_path}")
                    
                    # T√¨m file chromedriver th·∫≠t trong c√πng th∆∞ m·ª•c
                    wdm_dir = Path(wdm_path).parent
                    possible_paths = [
                        wdm_dir / "chromedriver",
                        wdm_dir / "chromedriver-linux64" / "chromedriver",
                        wdm_dir.parent / "chromedriver",
                    ]
                    
                    for possible_path in possible_paths:
                        if possible_path.exists() and possible_path.name == 'chromedriver':
                            # ƒê·∫£m b·∫£o file c√≥ quy·ªÅn th·ª±c thi
                            if not os.access(possible_path, os.X_OK):
                                self.logger.info(f"üîß ƒê·∫∑t quy·ªÅn th·ª±c thi cho: {possible_path}")
                                os.chmod(possible_path, 0o755)
                            
                            if os.access(possible_path, os.X_OK):
                                driver_path = str(possible_path)
                                self.logger.info(f"‚úÖ T√¨m th·∫•y ChromeDriver ƒë√∫ng: {driver_path}")
                                break
                    
                    if not driver_path:
                        self.logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file chromedriver th·∫≠t")
                        driver_path = None
                    
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è webdriver-manager th·∫•t b·∫°i: {e}")
                driver_path = None
            
            # Fallback: T√¨m chromedriver trong system PATH
            if not driver_path:
                self.logger.info("üîç T√¨m chromedriver trong system PATH...")
                import shutil
                system_driver = shutil.which('chromedriver')
                if system_driver:
                    driver_path = system_driver
                    self.logger.info(f"‚úÖ T√¨m th·∫•y chromedriver system: {driver_path}")
                else:
                    self.logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y chromedriver trong PATH")
            
            # Fallback: T·∫£i chromedriver th·ªß c√¥ng
            if not driver_path:
                self.logger.info("üì• T·∫£i chromedriver th·ªß c√¥ng...")
                driver_path = self.download_chromedriver_manual()
            
            # T·∫°o service v√† driver
            if driver_path:
                service = Service(driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.driver.set_page_load_timeout(config.BROWSER_TIMEOUT)
                
                self.logger.info("‚úÖ Chrome driver ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
                return True
            else:
                self.logger.error("‚ùå Kh√¥ng th·ªÉ t√¨m th·∫•y ChromeDriver")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi kh·ªüi t·∫°o Chrome driver: {e}")
            return False
    
    def download_chromedriver_manual(self):
        """T·∫£i ChromeDriver th·ªß c√¥ng n·∫øu webdriver-manager th·∫•t b·∫°i"""
        try:
            import os
            import requests
            import zipfile
            from pathlib import Path
            
            self.logger.info("üîÑ ƒêang t·∫£i ChromeDriver th·ªß c√¥ng...")
            
            # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ
            driver_dir = Path.home() / ".chrome_driver_manual"
            driver_dir.mkdir(exist_ok=True)
            
            # Ki·ªÉm tra xem ƒë√£ c√≥ file ch∆∞a
            chromedriver_path = driver_dir / "chromedriver"
            if chromedriver_path.exists() and os.access(chromedriver_path, os.X_OK):
                self.logger.info(f"‚úÖ S·ª≠ d·ª•ng ChromeDriver ƒë√£ c√≥: {chromedriver_path}")
                return str(chromedriver_path)
            
            # T·∫£i ChromeDriver m·ªõi
            chrome_version = "120.0.6099.71"  # Stable version
            download_url = f"https://chromedriver.storage.googleapis.com/{chrome_version}/chromedriver_linux64.zip"
            
            self.logger.info(f"üì• T·∫£i t·ª´: {download_url}")
            
            response = requests.get(download_url, timeout=30)
            if response.status_code == 200:
                # L∆∞u v√† gi·∫£i n√©n
                zip_file = driver_dir / "chromedriver.zip"
                with open(zip_file, 'wb') as f:
                    f.write(response.content)
                
                with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                    zip_ref.extractall(driver_dir)
                
                # ƒê·∫∑t quy·ªÅn executable
                os.chmod(chromedriver_path, 0o755)
                zip_file.unlink()  # X√≥a file zip
                
                self.logger.info(f"‚úÖ ChromeDriver ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ: {chromedriver_path}")
                return str(chromedriver_path)
            else:
                self.logger.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i ChromeDriver: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi t·∫£i ChromeDriver th·ªß c√¥ng: {e}")
            return None
    
    def extract_user_info(self):
        """Extract user ID v√† username t·ª´ profile URL"""
        try:
            self.logger.info(f"üîç ƒêang extract th√¥ng tin user t·ª´: {config.TARGET_URL}")
            
            # Extract user ID t·ª´ URL: members/lamnguyenphu.493993/ ‚Üí 493993
            url_parts = config.TARGET_URL.split('/')
            username_with_id = url_parts[-2] if url_parts[-2] else url_parts[-3]
            
            # Extract user ID (ph·∫ßn sau d·∫•u .)
            if '.' in username_with_id:
                parts = username_with_id.split('.')
                self.username = parts[0]  # lamnguyenphu
                self.user_id = parts[1]   # 493993
            else:
                self.logger.error("‚ùå Kh√¥ng th·ªÉ extract user ID t·ª´ URL")
                return False
            
            self.logger.info(f"‚úÖ Username: {self.username}")
            self.logger.info(f"‚úÖ User ID: {self.user_id}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi extract user info: {e}")
            return False
    
    def construct_search_url(self):
        """Construct URL t√¨m ki·∫øm t·∫•t c·∫£ b√†i ƒëƒÉng c·ªßa user"""
        try:
            # URL search format: https://f319.com/search/member?user_id=493993
            base_domain = "https://f319.com"
            search_url = f"{base_domain}/search/member?user_id={self.user_id}"
            
            self.search_base_url = search_url
            self.logger.info(f"üéØ Search URL: {search_url}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi construct search URL: {e}")
            return False
    
    def navigate_to_search(self):
        """Navigate t·ªõi trang search"""
        try:
            self.logger.info(f"üìã ƒêang truy c·∫≠p search page...")
            self.driver.get(self.search_base_url)
            time.sleep(3)
            
            # Ki·ªÉm tra xem c√≥ load th√†nh c√¥ng kh√¥ng
            if "search" in self.driver.current_url.lower():
                self.logger.info("‚úÖ ƒê√£ truy c·∫≠p th√†nh c√¥ng search page!")
                return True
            else:
                self.logger.error("‚ùå Kh√¥ng th·ªÉ truy c·∫≠p search page")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi navigate t·ªõi search: {e}")
            return False
    
    def detect_pagination(self):
        """Detect t·ªïng s·ªë trang t·ª´ pagination"""
        try:
            self.logger.info("üìä ƒêang detect pagination...")
            
            # T√¨m pagination element
            pagination_selectors = [
                ".PageNav",
                ".pageNav",
                "[data-last]",
                ".pagination"
            ]
            
            total_pages = 1  # Default l√† 1 page
            
            for selector in pagination_selectors:
                try:
                    pagination = self.driver.find_element(By.CSS_SELECTOR, selector)
                    
                    # Th·ª≠ l·∫•y t·ª´ data-last attribute
                    data_last = pagination.get_attribute('data-last')
                    if data_last and data_last.isdigit():
                        total_pages = int(data_last)
                        self.logger.info(f"‚úÖ T√¨m th·∫•y pagination t·ª´ data-last: {total_pages} trang")
                        break
                    
                    # Th·ª≠ t√¨m t·ª´ text content
                    page_text = pagination.text
                    page_match = re.search(r'Trang\s+\d+/(\d+)', page_text)
                    if page_match:
                        total_pages = int(page_match.group(1))
                        self.logger.info(f"‚úÖ T√¨m th·∫•y pagination t·ª´ text: {total_pages} trang")
                        break
                    
                except:
                    continue
            
            self.logger.info(f"üìÑ T·ªïng s·ªë trang ƒë·ªÉ crawl: {total_pages}")
            return total_pages
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Kh√¥ng detect ƒë∆∞·ª£c pagination: {e}")
            return 1
    
    def extract_posts_from_search_page(self, page_num=1):
        """Extract t·∫•t c·∫£ posts t·ª´ search results page"""
        try:
            self.logger.info(f"üìã ƒêang extract posts t·ª´ page {page_num}...")
            posts_data = []
            
            # T√¨m search results container
            results_selectors = [
                "ol.searchResultsList li.searchResult",
                ".searchResults li.searchResult", 
                ".searchResultsList .searchResult",
                "[data-author]"
            ]
            
            search_results = []
            for selector in results_selectors:
                try:
                    results = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if results:
                        search_results = results
                        self.logger.info(f"‚úÖ T√¨m th·∫•y {len(results)} results v·ªõi selector: {selector}")
                        break
                except:
                    continue
            
            if not search_results:
                self.logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y search results tr√™n page {page_num}")
                return posts_data
            
            # Extract t·ª´ng post
            for i, result in enumerate(search_results, 1):
                try:
                    # Ki·ªÉm tra author
                    author = result.get_attribute('data-author')
                    if not author or author.lower() != self.username.lower():
                        continue
                    
                    # Extract post ID
                    post_id = result.get_attribute('id')
                    if post_id:
                        post_id = post_id.replace('post-', '')
                    
                    # Extract title v√† URL
                    title_element = result.find_element(By.CSS_SELECTOR, "h3.title a, .title a")
                    post_title = title_element.text.strip()
                    post_url = title_element.get_attribute('href')
                    
                    # Convert relative URL to absolute
                    if post_url.startswith('posts/'):
                        post_url = f"https://f319.com/{post_url}"
                    elif post_url.startswith('/posts/'):
                        post_url = f"https://f319.com{post_url}"
                    
                    # Extract snippet content
                    snippet_content = "N/A"
                    try:
                        snippet_element = result.find_element(By.CSS_SELECTOR, "blockquote.snippet, .snippet")
                        snippet_content = snippet_element.text.strip()
                    except:
                        pass
                    
                    # Extract time
                    post_time = "N/A"
                    try:
                        time_selectors = [
                            ".DateTime",
                            "[data-time]",
                            ".meta time",
                            "abbr.DateTime"
                        ]
                        for time_sel in time_selectors:
                            try:
                                time_element = result.find_element(By.CSS_SELECTOR, time_sel)
                                post_time = time_element.get_attribute('datetime') or \
                                           time_element.get_attribute('title') or \
                                           time_element.text.strip()
                                if post_time:
                                    break
                            except:
                                continue
                    except:
                        pass
                    
                    post_data = {
                        'post_id': post_id,
                        'post_url': post_url,
                        'post_title': post_title,
                        'comment_content': snippet_content,
                        'comment_time': post_time,
                        'comment_link': post_url,
                        'author': author,
                        'page_number': page_num,
                        'source': 'search_snippet'
                    }
                    
                    posts_data.append(post_data)
                    self.logger.info(f"‚úÖ [{page_num}.{i}] Extracted: {post_title[:50]}...")
                    
                except Exception as e:
                    self.logger.debug(f"‚ö†Ô∏è L·ªói khi extract post {i}: {e}")
                    continue
            
            self.logger.info(f"üìä Page {page_num}: Extracted {len(posts_data)} posts")
            return posts_data
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi extract posts t·ª´ page {page_num}: {e}")
            return []
    
    def crawl_all_pages(self):
        """Crawl t·∫•t c·∫£ pages trong search results"""
        try:
            all_posts = []
            
            # Detect total pages
            total_pages = self.detect_pagination()
            
            if config.MAX_POSTS_TO_CRAWL > 0:
                max_pages = min(total_pages, (config.MAX_POSTS_TO_CRAWL // 20) + 1)  # ~20 posts per page
                total_pages = max_pages
                self.logger.info(f"üî¢ Gi·ªõi h·∫°n crawl: {total_pages} trang ƒë·∫ßu ti√™n")
            
            # Crawl t·ª´ng page
            for page in range(1, total_pages + 1):
                self.logger.info(f"üìñ [{page}/{total_pages}] ƒêang crawl page {page}...")
                
                # Navigate t·ªõi page c·ª• th·ªÉ n·∫øu kh√¥ng ph·∫£i page 1
                if page > 1:
                    page_url = f"{self.search_base_url}&page={page}"
                    self.driver.get(page_url)
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
                
                # Extract posts t·ª´ page n√†y
                page_posts = self.extract_posts_from_search_page(page)
                all_posts.extend(page_posts)
                
                # Check limit
                if config.MAX_POSTS_TO_CRAWL > 0 and len(all_posts) >= config.MAX_POSTS_TO_CRAWL:
                    all_posts = all_posts[:config.MAX_POSTS_TO_CRAWL]
                    self.logger.info(f"üî¢ ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {config.MAX_POSTS_TO_CRAWL} posts")
                    break
                
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            self.comments_data = all_posts
            self.logger.info(f"üéâ Ho√†n th√†nh crawl! T·ªïng c·ªông: {len(all_posts)} posts")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi crawl all pages: {e}")
            return False
    
    def get_full_content_option(self, post_data):
        """Option ƒë·ªÉ l·∫•y full content t·ª´ post URL (ch·ªâ c·ªßa user target)"""
        try:
            self.logger.info(f"üìù ƒêang l·∫•y full content t·ª´: {post_data['post_url']}")
            
            self.driver.get(post_data['post_url'])
            time.sleep(2)
            
            # ‚ö†Ô∏è FIX: Ch·ªâ l·∫•y content t·ª´ comments c·ªßa user target
            # T√¨m t·∫•t c·∫£ comments c·ªßa user target trong post n√†y
            target_comments = []
            comment_selectors = [
                f"li.message[data-author='{self.username}']",
                f"[data-author='{self.username}']",
                f".message[data-author='{self.username}']"
            ]
            
            for selector in comment_selectors:
                try:
                    comments = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if comments:
                        target_comments = comments
                        self.logger.info(f"‚úÖ T√¨m th·∫•y {len(comments)} comments c·ªßa {self.username} v·ªõi selector: {selector}")
                        break
                except:
                    continue
            
            if not target_comments:
                self.logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y comment n√†o c·ªßa {self.username} trong post n√†y")
                return post_data
            
            # L·∫•y content t·ª´ comment ƒë·∫ßu ti√™n c·ªßa user (ho·∫∑c t·∫•t c·∫£ n·∫øu c√≥ nhi·ªÅu)
            full_content_parts = []
            
            for i, comment in enumerate(target_comments):
                try:
                    # Verify l·∫°i author ƒë·ªÉ ch·∫Øc ch·∫Øn
                    comment_author = comment.get_attribute('data-author')
                    if comment_author and comment_author.lower() != self.username.lower():
                        self.logger.warning(f"‚ö†Ô∏è Skip comment v·ªõi author kh√°c: {comment_author}")
                        continue
                    
                    # Extract content t·ª´ comment n√†y
                    content_selectors = [
                        ".messageText.ugc.baseHtml",
                        ".messageText", 
                        ".message-body .bbWrapper",
                        ".post-content"
                    ]
                    
                    comment_content = ""
                    for content_sel in content_selectors:
                        try:
                            content_element = comment.find_element(By.CSS_SELECTOR, content_sel)
                            comment_content = content_element.text.strip()
                            if comment_content:
                                break
                        except:
                            continue
                    
                    if comment_content:
                        full_content_parts.append(comment_content)
                        self.logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c content t·ª´ comment #{i+1} c·ªßa {self.username}")
                
                except Exception as e:
                    self.logger.debug(f"‚ö†Ô∏è L·ªói khi extract content t·ª´ comment #{i+1}: {e}")
                    continue
            
            # Combine all content parts
            if full_content_parts:
                # N·∫øu c√≥ nhi·ªÅu comments, k·∫øt h·ª£p l·∫°i
                full_content = "\n--- G·ªôp b√†i vi·∫øt ---\n".join(full_content_parts)
                
                # Ch·ªâ update n·∫øu full content d√†i h∆°n snippet content
                current_content = post_data.get('comment_content', '')
                if len(full_content) > len(current_content):
                    post_data['comment_content'] = full_content
                    post_data['source'] = 'full_content'
                    self.logger.info(f"‚úÖ ƒê√£ update v·ªõi full content ({len(full_content)} chars)")
                else:
                    self.logger.info(f"‚ö° Snippet content ƒë√£ ƒë·ªß chi ti·∫øt")
            else:
                self.logger.warning(f"‚ö†Ô∏è Kh√¥ng extract ƒë∆∞·ª£c content t·ª´ comment n√†o")
            
            return post_data
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y full content: {e}")
            return post_data
    
    def save_to_csv(self):
        """L∆∞u d·ªØ li·ªáu v√†o file CSV"""
        try:
            if not self.comments_data:
                self.logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
                return False
            
            output_file = config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')
            self.logger.info(f"üíæ ƒêang l∆∞u {len(self.comments_data)} posts v√†o {output_file}")
            
            # FIX: Th√™m UTF-8 BOM ƒë·ªÉ Excel t·ª± ƒë·ªông detect encoding
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['post_id', 'post_url', 'post_title', 'comment_content', 'comment_time', 
                             'comment_link', 'author', 'page_number', 'source']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for post in self.comments_data:
                    writer.writerow(post)
            
            self.logger.info(f"‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng d·ªØ li·ªáu v√†o {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi l∆∞u file CSV: {e}")
            return False
    
    def cleanup(self):
        """D·ªçn d·∫πp resources"""
        if self.driver:
            self.driver.quit()
            self.logger.info("üîö ƒê√£ ƒë√≥ng browser")
    
    def run(self, get_full_content=False):
        """Ch·∫°y enhanced crawler ch√≠nh"""
        try:
            self.logger.info("üöÄ === B·∫Øt ƒë·∫ßu Enhanced F319 Crawler ===")
            self.logger.info("üéØ S·ª≠ d·ª•ng Search-based approach")
            
            # Setup driver
            if not self.setup_driver():
                return False
            
            # Extract user info
            if not self.extract_user_info():
                return False
            
            # Construct search URL
            if not self.construct_search_url():
                return False
            
            # Navigate t·ªõi search page
            if not self.navigate_to_search():
                return False
            
            # Crawl all pages
            if not self.crawl_all_pages():
                return False
            
            # Option: L·∫•y full content
            if get_full_content and self.comments_data:
                self.logger.info("üìù === B·∫Øt ƒë·∫ßu l·∫•y full content ===")
                
                # X√°c ƒë·ªãnh s·ªë posts c·∫ßn l·∫•y full content
                total_posts = len(self.comments_data)
                
                if config.MAX_FULL_CONTENT_POSTS == -1:
                    # Kh√¥ng l·∫•y full content
                    self.logger.info("‚ö†Ô∏è MAX_FULL_CONTENT_POSTS = -1, b·ªè qua l·∫•y full content")
                elif config.MAX_FULL_CONTENT_POSTS == 0:
                    # L·∫•y t·∫•t c·∫£
                    posts_to_process = total_posts
                    self.logger.info(f"üìã S·∫Ω l·∫•y full content cho T·∫§T C·∫¢ {posts_to_process} posts")
                else:
                    # L·∫•y s·ªë l∆∞·ª£ng gi·ªõi h·∫°n
                    posts_to_process = min(config.MAX_FULL_CONTENT_POSTS, total_posts)
                    self.logger.info(f"üìã S·∫Ω l·∫•y full content cho {posts_to_process}/{total_posts} posts")
                
                # L·∫•y full content cho t·ª´ng post
                if config.MAX_FULL_CONTENT_POSTS != -1:
                    for i in range(posts_to_process):
                        post_data = self.comments_data[i]
                        
                        # Ki·ªÉm tra c√≥ n√™n skip post qu√° d√†i kh√¥ng
                        if config.SKIP_LONG_POSTS:
                            current_content = post_data.get('comment_content', '')
                            if len(current_content) > config.LONG_POST_THRESHOLD:
                                self.logger.info(f"‚è≠Ô∏è [{i+1}/{posts_to_process}] Skip post d√†i: {post_data.get('post_title', '')[:50]}...")
                                continue
                        
                        self.logger.info(f"üìù [{i+1}/{posts_to_process}] L·∫•y full content: {post_data.get('post_title', '')[:50]}...")
                        
                        # L·∫•y full content
                        self.comments_data[i] = self.get_full_content_option(post_data)
                        
                        # Delay gi·ªØa c√°c requests
                        if i < posts_to_process - 1:  # Kh√¥ng delay ·ªü post cu·ªëi c√πng
                            time.sleep(config.FULL_CONTENT_DELAY)
                
                self.logger.info("‚úÖ === Ho√†n th√†nh l·∫•y full content ===")
            
            # L∆∞u d·ªØ li·ªáu
            self.save_to_csv()
            
            self.logger.info(f"üéâ === Ho√†n th√†nh! ƒê√£ crawl {len(self.comments_data)} posts ===")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh crawl: {e}")
            return False
        
        finally:
            self.cleanup()

def main():
    """Function ch√≠nh"""
    print("üöÄ === Enhanced F319 Comment Crawler ===")
    print("üéØ Search-based approach - Nhanh & Hi·ªáu qu·∫£")
    print(f"üìã Target URL: {config.TARGET_URL}")
    print(f"üìÅ Output file: {config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')}")
    print(f"üî¢ Max posts: {config.MAX_POSTS_TO_CRAWL if config.MAX_POSTS_TO_CRAWL > 0 else 'Kh√¥ng gi·ªõi h·∫°n'}")
    print("=" * 60)
    
    # Option ƒë·ªÉ l·∫•y full content
    full_content_choice = input("ü§î L·∫•y full content t·ª´ t·ª´ng post? (y/n, default=n): ").strip().lower()
    get_full_content = full_content_choice in ['y', 'yes']
    
    if get_full_content:
        print("üìù S·∫Ω l·∫•y full content (ch·∫≠m h∆°n nh∆∞ng chi ti·∫øt h∆°n)")
    else:
        print("‚ö° Ch·ªâ l·∫•y snippet content (nhanh h∆°n)")
    
    print("=" * 60)
    
    crawler = EnhancedF319Crawler()
    success = crawler.run(get_full_content=get_full_content)
    
    if success:
        output_file = config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')
        print(f"\n‚úÖ Enhanced Crawler ho√†n th√†nh th√†nh c√¥ng!")
        print(f"üìÅ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u trong file: {output_file}")
        print(f"üìä Logs: enhanced_crawler.log")
    else:
        print(f"\n‚ùå Crawler g·∫∑p l·ªói. Ki·ªÉm tra file log ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.")

if __name__ == "__main__":
    main() 