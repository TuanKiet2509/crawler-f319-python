# -*- coding: utf-8 -*-
"""
Enhanced F319 Comment Crawler
Sá»­ dá»¥ng search approach hiá»‡u quáº£ hÆ¡n
"""

import time
import csv
import sys
import logging
import re
import zipfile
from datetime import datetime
from urllib.parse import urljoin, urlparse, parse_qs
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import config

class EnhancedF319Crawler:
    def __init__(self):
        self.driver = None
        self.comments_data = []
        self.search_base_url = None
        self.user_id = None
        self.username = None
        self.setup_logging()
        
    def setup_logging(self):
        """Thiáº¿t láº­p logging"""
        if config.ENABLE_LOGGING:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler('enhanced_crawler.log', encoding='utf-8'),
                    logging.StreamHandler(sys.stdout)
                ]
            )
        self.logger = logging.getLogger(__name__)
        
    def setup_driver(self):
        """Khá»Ÿi táº¡o Chrome driver"""
        try:
            self.logger.info("ğŸš€ Äang khá»Ÿi táº¡o Chrome driver...")
            chrome_options = Options()
            
            if config.HEADLESS_MODE:
                chrome_options.add_argument("--headless")
            
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            # FIX: ThÃªm fallback options khi webdriver-manager tháº¥t báº¡i
            driver_path = None
            
            try:
                # Thá»­ webdriver-manager trÆ°á»›c
                self.logger.info("ğŸ”§ Thá»­ sá»­ dá»¥ng webdriver-manager...")
                wdm_path = ChromeDriverManager().install()
                
                # FIX: Kiá»ƒm tra xem WebDriver Manager cÃ³ tráº£ vá» Ä‘Ãºng file khÃ´ng
                import os
                from pathlib import Path
                
                # Kiá»ƒm tra xem file cÃ³ pháº£i lÃ  chromedriver tháº­t khÃ´ng
                # KhÃ´ng chá»‰ kiá»ƒm tra tÃªn file mÃ  cÃ²n kiá»ƒm tra cÃ³ thá»ƒ thá»±c thi
                is_valid_chromedriver = False
                
                # Kiá»ƒm tra náº¿u file cÃ³ tÃªn chÃ­nh xÃ¡c lÃ  'chromedriver' (khÃ´ng cÃ³ extension)
                if Path(wdm_path).name == 'chromedriver':
                    if os.path.exists(wdm_path) and os.access(wdm_path, os.X_OK):
                        driver_path = wdm_path
                        is_valid_chromedriver = True
                        self.logger.info(f"âœ… Sá»­ dá»¥ng ChromeDriver tá»« webdriver-manager: {driver_path}")
                
                # Náº¿u WebDriver Manager tráº£ vá» file sai
                if not is_valid_chromedriver:
                    self.logger.warning(f"âš ï¸ WebDriver Manager tráº£ vá» file khÃ´ng há»£p lá»‡: {wdm_path}")
                    
                    # TÃ¬m file chromedriver tháº­t trong cÃ¹ng thÆ° má»¥c
                    wdm_dir = Path(wdm_path).parent
                    possible_paths = [
                        wdm_dir / "chromedriver",
                        wdm_dir / "chromedriver-linux64" / "chromedriver",
                        wdm_dir.parent / "chromedriver",
                    ]
                    
                    for possible_path in possible_paths:
                        if possible_path.exists() and possible_path.name == 'chromedriver':
                            # Äáº£m báº£o file cÃ³ quyá»n thá»±c thi
                            if not os.access(possible_path, os.X_OK):
                                self.logger.info(f"ğŸ”§ Äáº·t quyá»n thá»±c thi cho: {possible_path}")
                                os.chmod(possible_path, 0o755)
                            
                            if os.access(possible_path, os.X_OK):
                                driver_path = str(possible_path)
                                self.logger.info(f"âœ… TÃ¬m tháº¥y ChromeDriver Ä‘Ãºng: {driver_path}")
                                break
                    
                    if not driver_path:
                        self.logger.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file chromedriver tháº­t")
                        driver_path = None
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ webdriver-manager tháº¥t báº¡i: {e}")
                driver_path = None
            
            # Fallback: TÃ¬m chromedriver trong system PATH
            if not driver_path:
                self.logger.info("ğŸ” TÃ¬m chromedriver trong system PATH...")
                import shutil
                system_driver = shutil.which('chromedriver')
                if system_driver:
                    driver_path = system_driver
                    self.logger.info(f"âœ… TÃ¬m tháº¥y chromedriver system: {driver_path}")
                else:
                    self.logger.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y chromedriver trong PATH")
            
            # Fallback: Táº£i chromedriver thá»§ cÃ´ng
            if not driver_path:
                self.logger.info("ğŸ“¥ Táº£i chromedriver thá»§ cÃ´ng...")
                driver_path = self.download_chromedriver_manual()
            
            # Táº¡o service vÃ  driver
            if driver_path:
                service = Service(driver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
                self.driver.set_page_load_timeout(config.BROWSER_TIMEOUT)
                
                self.logger.info("âœ… Chrome driver Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng!")
                return True
            else:
                self.logger.error("âŒ KhÃ´ng thá»ƒ tÃ¬m tháº¥y ChromeDriver")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi khá»Ÿi táº¡o Chrome driver: {e}")
            return False
    
    def download_chromedriver_manual(self):
        """Táº£i ChromeDriver thá»§ cÃ´ng náº¿u webdriver-manager tháº¥t báº¡i"""
        try:
            import os
            import requests
            import zipfile
            from pathlib import Path
            
            self.logger.info("ğŸ”„ Äang táº£i ChromeDriver thá»§ cÃ´ng...")
            
            # Táº¡o thÆ° má»¥c lÆ°u trá»¯
            driver_dir = Path.home() / ".chrome_driver_manual"
            driver_dir.mkdir(exist_ok=True)
            
            # Kiá»ƒm tra xem Ä‘Ã£ cÃ³ file chÆ°a
            chromedriver_path = driver_dir / "chromedriver"
            if chromedriver_path.exists() and os.access(chromedriver_path, os.X_OK):
                self.logger.info(f"âœ… Sá»­ dá»¥ng ChromeDriver Ä‘Ã£ cÃ³: {chromedriver_path}")
                return str(chromedriver_path)
            
            # Táº£i ChromeDriver má»›i
            chrome_version = "120.0.6099.71"  # Stable version
            download_url = f"https://chromedriver.storage.googleapis.com/{chrome_version}/chromedriver_linux64.zip"
            
            self.logger.info(f"ğŸ“¥ Táº£i tá»«: {download_url}")
            
            response = requests.get(download_url, timeout=30)
            if response.status_code == 200:
                # LÆ°u vÃ  giáº£i nÃ©n
                zip_file = driver_dir / "chromedriver.zip"
                with open(zip_file, 'wb') as f:
                    f.write(response.content)
                
                with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                    zip_ref.extractall(driver_dir)
                
                # Äáº·t quyá»n executable
                os.chmod(chromedriver_path, 0o755)
                zip_file.unlink()  # XÃ³a file zip
                
                self.logger.info(f"âœ… ChromeDriver Ä‘Ã£ Ä‘Æ°á»£c táº£i vá»: {chromedriver_path}")
                return str(chromedriver_path)
            else:
                self.logger.error(f"âŒ KhÃ´ng thá»ƒ táº£i ChromeDriver: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi táº£i ChromeDriver thá»§ cÃ´ng: {e}")
            return None
    
    def extract_user_info(self):
        """Extract user ID vÃ  username tá»« profile URL"""
        try:
            self.logger.info(f"ğŸ” Äang extract thÃ´ng tin user tá»«: {config.TARGET_URL}")
            
            # Extract user ID tá»« URL: members/lamnguyenphu.493993/ â†’ 493993
            url_parts = config.TARGET_URL.split('/')
            username_with_id = url_parts[-2] if url_parts[-2] else url_parts[-3]
            
            # Extract user ID (pháº§n sau dáº¥u .)
            if '.' in username_with_id:
                parts = username_with_id.split('.')
                self.username = parts[0]  # lamnguyenphu
                self.user_id = parts[1]   # 493993
            else:
                self.logger.error("âŒ KhÃ´ng thá»ƒ extract user ID tá»« URL")
                return False
            
            self.logger.info(f"âœ… Username: {self.username}")
            self.logger.info(f"âœ… User ID: {self.user_id}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi extract user info: {e}")
            return False
    
    def construct_search_url(self):
        """Construct URL tÃ¬m kiáº¿m táº¥t cáº£ bÃ i Ä‘Äƒng cá»§a user"""
        try:
            # URL search format: https://f319.com/search/member?user_id=493993
            base_domain = "https://f319.com"
            search_url = f"{base_domain}/search/member?user_id={self.user_id}"
            
            self.search_base_url = search_url
            self.logger.info(f"ğŸ¯ Search URL: {search_url}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi construct search URL: {e}")
            return False
    
    def navigate_to_search(self):
        """Navigate tá»›i trang search"""
        try:
            self.logger.info(f"ğŸ“‹ Äang truy cáº­p search page...")
            self.driver.get(self.search_base_url)
            time.sleep(3)
            
            # Kiá»ƒm tra xem cÃ³ load thÃ nh cÃ´ng khÃ´ng
            if "search" in self.driver.current_url.lower():
                self.logger.info("âœ… ÄÃ£ truy cáº­p thÃ nh cÃ´ng search page!")
                return True
            else:
                self.logger.error("âŒ KhÃ´ng thá»ƒ truy cáº­p search page")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi navigate tá»›i search: {e}")
            return False
    
    def detect_pagination(self):
        """Detect tá»•ng sá»‘ trang tá»« pagination"""
        try:
            self.logger.info("ğŸ“Š Äang detect pagination...")
            
            # TÃ¬m pagination element
            pagination_selectors = [
                ".PageNav",
                ".pageNav",
                "[data-last]",
                ".pagination"
            ]
            
            total_pages = 1  # Default lÃ  1 page
            
            for selector in pagination_selectors:
                try:
                    pagination = self.driver.find_element(By.CSS_SELECTOR, selector)
                    
                    # Thá»­ láº¥y tá»« data-last attribute
                    data_last = pagination.get_attribute('data-last')
                    if data_last and data_last.isdigit():
                        total_pages = int(data_last)
                        self.logger.info(f"âœ… TÃ¬m tháº¥y pagination tá»« data-last: {total_pages} trang")
                        break
                    
                    # Thá»­ tÃ¬m tá»« text content
                    page_text = pagination.text
                    page_match = re.search(r'Trang\s+\d+/(\d+)', page_text)
                    if page_match:
                        total_pages = int(page_match.group(1))
                        self.logger.info(f"âœ… TÃ¬m tháº¥y pagination tá»« text: {total_pages} trang")
                        break
                    
                except:
                    continue
            
            self.logger.info(f"ğŸ“„ Tá»•ng sá»‘ trang Ä‘á»ƒ crawl: {total_pages}")
            return total_pages
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ KhÃ´ng detect Ä‘Æ°á»£c pagination: {e}")
            return 1
    
    def extract_posts_from_search_page(self, page_num=1):
        """Extract táº¥t cáº£ posts tá»« search results page"""
        try:
            self.logger.info(f"ğŸ“‹ Äang extract posts tá»« page {page_num}...")
            posts_data = []
            
            # TÃ¬m search results container
            results_selectors = [
                "ol.searchResultsList li.searchResult",
                ".searchResults li.searchResult", 
                ".searchResultsList .searchResult",
                "[data-author]"
            ]
            
            search_results = []
            for selector in results_selectors:
                try:
                    results = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if results:
                        search_results = results
                        self.logger.info(f"âœ… TÃ¬m tháº¥y {len(results)} results vá»›i selector: {selector}")
                        break
                except:
                    continue
            
            if not search_results:
                self.logger.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y search results trÃªn page {page_num}")
                return posts_data
            
            # Extract tá»«ng post
            for i, result in enumerate(search_results, 1):
                try:
                    # Kiá»ƒm tra author
                    author = result.get_attribute('data-author')
                    if not author or author.lower() != self.username.lower():
                        continue
                    
                    # Extract post ID
                    post_id = result.get_attribute('id')
                    if post_id:
                        post_id = post_id.replace('post-', '')
                    
                    # Extract title vÃ  URL
                    title_element = result.find_element(By.CSS_SELECTOR, "h3.title a, .title a")
                    post_title = title_element.text.strip()
                    post_url = title_element.get_attribute('href')
                    
                    # Convert relative URL to absolute
                    if post_url.startswith('posts/'):
                        post_url = f"https://f319.com/{post_url}"
                    elif post_url.startswith('/posts/'):
                        post_url = f"https://f319.com{post_url}"
                    
                    # Extract snippet content
                    snippet_content = "N/A"
                    try:
                        snippet_element = result.find_element(By.CSS_SELECTOR, "blockquote.snippet, .snippet")
                        snippet_content = snippet_element.text.strip()
                    except:
                        pass
                    
                    # Extract time
                    post_time = "N/A"
                    try:
                        time_selectors = [
                            ".DateTime",
                            "[data-time]",
                            ".meta time",
                            "abbr.DateTime"
                        ]
                        for time_sel in time_selectors:
                            try:
                                time_element = result.find_element(By.CSS_SELECTOR, time_sel)
                                post_time = time_element.get_attribute('datetime') or \
                                           time_element.get_attribute('title') or \
                                           time_element.text.strip()
                                if post_time:
                                    break
                            except:
                                continue
                    except:
                        pass
                    
                    post_data = {
                        'post_id': post_id,
                        'post_url': post_url,
                        'post_title': post_title,
                        'comment_content': snippet_content,
                        'comment_time': post_time,
                        'comment_link': post_url,
                        'author': author,
                        'page_number': page_num,
                        'source': 'search_snippet'
                    }
                    
                    posts_data.append(post_data)
                    self.logger.info(f"âœ… [{page_num}.{i}] Extracted: {post_title[:50]}...")
                    
                except Exception as e:
                    self.logger.debug(f"âš ï¸ Lá»—i khi extract post {i}: {e}")
                    continue
            
            self.logger.info(f"ğŸ“Š Page {page_num}: Extracted {len(posts_data)} posts")
            return posts_data
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi extract posts tá»« page {page_num}: {e}")
            return []
    
    def crawl_all_pages(self):
        """Crawl táº¥t cáº£ pages trong search results"""
        try:
            all_posts = []
            
            # Detect total pages
            total_pages = self.detect_pagination()
            
            if config.MAX_POSTS_TO_CRAWL > 0:
                max_pages = min(total_pages, (config.MAX_POSTS_TO_CRAWL // 20) + 1)  # ~20 posts per page
                total_pages = max_pages
                self.logger.info(f"ğŸ”¢ Giá»›i háº¡n crawl: {total_pages} trang Ä‘áº§u tiÃªn")
            
            # Crawl tá»«ng page
            for page in range(1, total_pages + 1):
                self.logger.info(f"ğŸ“– [{page}/{total_pages}] Äang crawl page {page}...")
                
                # Navigate tá»›i page cá»¥ thá»ƒ náº¿u khÃ´ng pháº£i page 1
                if page > 1:
                    page_url = f"{self.search_base_url}&page={page}"
                    self.driver.get(page_url)
                    time.sleep(config.DELAY_BETWEEN_REQUESTS)
                
                # Extract posts tá»« page nÃ y
                page_posts = self.extract_posts_from_search_page(page)
                all_posts.extend(page_posts)
                
                # Check limit
                if config.MAX_POSTS_TO_CRAWL > 0 and len(all_posts) >= config.MAX_POSTS_TO_CRAWL:
                    all_posts = all_posts[:config.MAX_POSTS_TO_CRAWL]
                    self.logger.info(f"ğŸ”¢ ÄÃ£ Ä‘áº¡t giá»›i háº¡n {config.MAX_POSTS_TO_CRAWL} posts")
                    break
                
                time.sleep(config.DELAY_BETWEEN_REQUESTS)
            
            self.comments_data = all_posts
            self.logger.info(f"ğŸ‰ HoÃ n thÃ nh crawl! Tá»•ng cá»™ng: {len(all_posts)} posts")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi crawl all pages: {e}")
            return False
    
    def get_full_content_option(self, post_data):
        """Option Ä‘á»ƒ láº¥y full content tá»« post URL (chá»‰ cá»§a user target)"""
        try:
            self.logger.info(f"ğŸ“ Äang láº¥y full content tá»«: {post_data['post_url']}")
            
            self.driver.get(post_data['post_url'])
            time.sleep(2)
            
            # âš ï¸ FIX: Chá»‰ láº¥y content tá»« comments cá»§a user target
            # TÃ¬m táº¥t cáº£ comments cá»§a user target trong post nÃ y
            target_comments = []
            comment_selectors = [
                f"li.message[data-author='{self.username}']",
                f"[data-author='{self.username}']",
                f".message[data-author='{self.username}']"
            ]
            
            for selector in comment_selectors:
                try:
                    comments = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if comments:
                        target_comments = comments
                        self.logger.info(f"âœ… TÃ¬m tháº¥y {len(comments)} comments cá»§a {self.username} vá»›i selector: {selector}")
                        break
                except:
                    continue
            
            if not target_comments:
                self.logger.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y comment nÃ o cá»§a {self.username} trong post nÃ y")
                return post_data
            
            # Láº¥y content tá»« comment Ä‘áº§u tiÃªn cá»§a user (hoáº·c táº¥t cáº£ náº¿u cÃ³ nhiá»u)
            full_content_parts = []
            
            for i, comment in enumerate(target_comments):
                try:
                    # Verify láº¡i author Ä‘á»ƒ cháº¯c cháº¯n
                    comment_author = comment.get_attribute('data-author')
                    if comment_author and comment_author.lower() != self.username.lower():
                        self.logger.warning(f"âš ï¸ Skip comment vá»›i author khÃ¡c: {comment_author}")
                        continue
                    
                    # Extract content tá»« comment nÃ y
                    content_selectors = [
                        ".messageText.ugc.baseHtml",
                        ".messageText", 
                        ".message-body .bbWrapper",
                        ".post-content"
                    ]
                    
                    comment_content = ""
                    for content_sel in content_selectors:
                        try:
                            content_element = comment.find_element(By.CSS_SELECTOR, content_sel)
                            comment_content = content_element.text.strip()
                            if comment_content:
                                break
                        except:
                            continue
                    
                    if comment_content:
                        full_content_parts.append(comment_content)
                        self.logger.info(f"âœ… Láº¥y Ä‘Æ°á»£c content tá»« comment #{i+1} cá»§a {self.username}")
                
                except Exception as e:
                    self.logger.debug(f"âš ï¸ Lá»—i khi extract content tá»« comment #{i+1}: {e}")
                    continue
            
            # Combine all content parts
            if full_content_parts:
                # Náº¿u cÃ³ nhiá»u comments, káº¿t há»£p láº¡i
                full_content = "\n--- Gá»™p bÃ i viáº¿t ---\n".join(full_content_parts)
                
                # Chá»‰ update náº¿u full content dÃ i hÆ¡n snippet content
                current_content = post_data.get('comment_content', '')
                if len(full_content) > len(current_content):
                    post_data['comment_content'] = full_content
                    post_data['source'] = 'full_content'
                    self.logger.info(f"âœ… ÄÃ£ update vá»›i full content ({len(full_content)} chars)")
                else:
                    self.logger.info(f"âš¡ Snippet content Ä‘Ã£ Ä‘á»§ chi tiáº¿t")
            else:
                self.logger.warning(f"âš ï¸ KhÃ´ng extract Ä‘Æ°á»£c content tá»« comment nÃ o")
            
            return post_data
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ KhÃ´ng thá»ƒ láº¥y full content: {e}")
            return post_data
    
    def save_to_csv(self):
        """LÆ°u dá»¯ liá»‡u vÃ o file CSV"""
        try:
            if not self.comments_data:
                self.logger.warning("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u")
                return False
            
            output_file = config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')
            self.logger.info(f"ğŸ’¾ Äang lÆ°u {len(self.comments_data)} posts vÃ o {output_file}")
            
            # FIX: ThÃªm UTF-8 BOM Ä‘á»ƒ Excel tá»± Ä‘á»™ng detect encoding
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['post_id', 'post_url', 'post_title', 'comment_content', 'comment_time', 
                             'comment_link', 'author', 'page_number', 'source']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                for post in self.comments_data:
                    writer.writerow(post)
            
            self.logger.info(f"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng dá»¯ liá»‡u vÃ o {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi lÆ°u file CSV: {e}")
            return False
    
    def cleanup(self):
        """Dá»n dáº¹p resources"""
        if self.driver:
            self.driver.quit()
            self.logger.info("ğŸ”š ÄÃ£ Ä‘Ã³ng browser")
    
    def run(self, get_full_content=False):
        """Cháº¡y enhanced crawler chÃ­nh"""
        try:
            self.logger.info("ğŸš€ === Báº¯t Ä‘áº§u Enhanced F319 Crawler ===")
            self.logger.info("ğŸ¯ Sá»­ dá»¥ng Search-based approach")
            
            # Setup driver
            if not self.setup_driver():
                return False
            
            # Extract user info
            if not self.extract_user_info():
                return False
            
            # Construct search URL
            if not self.construct_search_url():
                return False
            
            # Navigate tá»›i search page
            if not self.navigate_to_search():
                return False
            
            # Crawl all pages
            if not self.crawl_all_pages():
                return False
            
            # Option: Láº¥y full content
            if get_full_content and self.comments_data:
                self.logger.info("ğŸ“ === Báº¯t Ä‘áº§u láº¥y full content ===")
                
                # XÃ¡c Ä‘á»‹nh sá»‘ posts cáº§n láº¥y full content
                total_posts = len(self.comments_data)
                
                if config.MAX_FULL_CONTENT_POSTS == -1:
                    # KhÃ´ng láº¥y full content
                    self.logger.info("âš ï¸ MAX_FULL_CONTENT_POSTS = -1, bá» qua láº¥y full content")
                elif config.MAX_FULL_CONTENT_POSTS == 0:
                    # Láº¥y táº¥t cáº£
                    posts_to_process = total_posts
                    self.logger.info(f"ğŸ“‹ Sáº½ láº¥y full content cho Táº¤T Cáº¢ {posts_to_process} posts")
                else:
                    # Láº¥y sá»‘ lÆ°á»£ng giá»›i háº¡n
                    posts_to_process = min(config.MAX_FULL_CONTENT_POSTS, total_posts)
                    self.logger.info(f"ğŸ“‹ Sáº½ láº¥y full content cho {posts_to_process}/{total_posts} posts")
                
                # Láº¥y full content cho tá»«ng post
                if config.MAX_FULL_CONTENT_POSTS != -1:
                    for i in range(posts_to_process):
                        post_data = self.comments_data[i]
                        
                        # Kiá»ƒm tra cÃ³ nÃªn skip post quÃ¡ dÃ i khÃ´ng
                        if config.SKIP_LONG_POSTS:
                            current_content = post_data.get('comment_content', '')
                            if len(current_content) > config.LONG_POST_THRESHOLD:
                                self.logger.info(f"â­ï¸ [{i+1}/{posts_to_process}] Skip post dÃ i: {post_data.get('post_title', '')[:50]}...")
                                continue
                        
                        self.logger.info(f"ğŸ“ [{i+1}/{posts_to_process}] Láº¥y full content: {post_data.get('post_title', '')[:50]}...")
                        
                        # Láº¥y full content
                        self.comments_data[i] = self.get_full_content_option(post_data)
                        
                        # Delay giá»¯a cÃ¡c requests
                        if i < posts_to_process - 1:  # KhÃ´ng delay á»Ÿ post cuá»‘i cÃ¹ng
                            time.sleep(config.FULL_CONTENT_DELAY)
                
                self.logger.info("âœ… === HoÃ n thÃ nh láº¥y full content ===")
            
            # LÆ°u dá»¯ liá»‡u
            self.save_to_csv()
            
            self.logger.info(f"ğŸ‰ === HoÃ n thÃ nh! ÄÃ£ crawl {len(self.comments_data)} posts ===")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh crawl: {e}")
            return False
        
        finally:
            self.cleanup()

def main():
    """Function chÃ­nh"""
    print("ğŸš€ === Enhanced F319 Comment Crawler ===")
    print("ğŸ¯ Search-based approach - Nhanh & Hiá»‡u quáº£")
    print(f"ğŸ“‹ Target URL: {config.TARGET_URL}")
    print(f"ğŸ“ Output file: {config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')}")
    print(f"ğŸ”¢ Max posts: {config.MAX_POSTS_TO_CRAWL if config.MAX_POSTS_TO_CRAWL > 0 else 'KhÃ´ng giá»›i háº¡n'}")
    print("=" * 60)
    
    # Option Ä‘á»ƒ láº¥y full content
    full_content_choice = input("ğŸ¤” Láº¥y full content tá»« tá»«ng post? (y/n, default=n): ").strip().lower()
    get_full_content = full_content_choice in ['y', 'yes']
    
    if get_full_content:
        print("ğŸ“ Sáº½ láº¥y full content (cháº­m hÆ¡n nhÆ°ng chi tiáº¿t hÆ¡n)")
    else:
        print("âš¡ Chá»‰ láº¥y snippet content (nhanh hÆ¡n)")
    
    print("=" * 60)
    
    crawler = EnhancedF319Crawler()
    success = crawler.run(get_full_content=get_full_content)
    
    if success:
        output_file = config.OUTPUT_FILE.replace('.csv', '_enhanced.csv')
        print(f"\nâœ… Enhanced Crawler hoÃ n thÃ nh thÃ nh cÃ´ng!")
        print(f"ğŸ“ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong file: {output_file}")
        print(f"ğŸ“Š Logs: enhanced_crawler.log")
    else:
        print(f"\nâŒ Crawler gáº·p lá»—i. Kiá»ƒm tra file log Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.")

if __name__ == "__main__":
    main() 