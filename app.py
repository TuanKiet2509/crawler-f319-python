# -*- coding: utf-8 -*-
"""
F319 Crawler Web Interface
Web app ƒë∆°n gi·∫£n ƒë·ªÉ ch·∫°y crawler v√† download k·∫øt qu·∫£
Enhanced v·ªõi multi-user support
"""

import os
import sys
import uuid
import threading
import time
from datetime import datetime, timedelta
from flask import Flask, render_template, request, jsonify, send_file, flash, redirect, url_for
import config
from enhanced_f319_crawler import EnhancedF319Crawler

app = Flask(__name__)
app.secret_key = 'f319_crawler_secret_key_2024'

# Global variables ƒë·ªÉ tracking crawl jobs
crawl_jobs = {}
DOWNLOADS_FOLDER = 'downloads'

# T·∫°o downloads folder n·∫øu ch∆∞a c√≥
if not os.path.exists(DOWNLOADS_FOLDER):
    os.makedirs(DOWNLOADS_FOLDER)

def cleanup_old_files():
    """D·ªçn d·∫πp files c≈© h∆°n 1 gi·ªù"""
    try:
        now = datetime.now()
        for filename in os.listdir(DOWNLOADS_FOLDER):
            file_path = os.path.join(DOWNLOADS_FOLDER, filename)
            if os.path.isfile(file_path):
                file_time = datetime.fromtimestamp(os.path.getctime(file_path))
                if now - file_time > timedelta(hours=1):
                    os.remove(file_path)
                    print(f"üóëÔ∏è ƒê√£ x√≥a file c≈©: {filename}")
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi cleanup files: {e}")

def parse_multiple_users(user_input):
    """Parse input ƒë·ªÉ l·∫•y danh s√°ch multiple users (comma-separated only)"""
    try:
        # Split by commas only (no more newlines since we use input field)
        users = []
        parts = user_input.strip().split(',')
        
        for part in parts:
            user = part.strip()
            if user and '.' in user:
                users.append(user)
        
        # Remove duplicates
        users = list(set(users))
        return users
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói parse users: {e}")
        return []

def run_single_user_crawler(username_with_id, output_filename, get_full_content=False):
    """Ch·∫°y crawler cho m·ªôt user"""
    try:
        # Construct target URL
        target_url = f"https://f319.com/members/{username_with_id}/"
        
        # Backup original config
        original_url = config.TARGET_URL
        original_output = config.OUTPUT_FILE
        original_headless = config.HEADLESS_MODE
        
        # Set config cho user n√†y
        config.TARGET_URL = target_url
        config.OUTPUT_FILE = output_filename
        config.HEADLESS_MODE = True  # Lu√¥n ch·∫°y headless tr√™n web
        
        # Ch·∫°y crawler
        crawler = EnhancedF319Crawler()
        success = crawler.run(get_full_content=get_full_content)
        
        # Restore original config
        config.TARGET_URL = original_url
        config.OUTPUT_FILE = original_output
        config.HEADLESS_MODE = original_headless
        
        return success
        
    except Exception as e:
        print(f"‚ùå L·ªói crawler cho user {username_with_id}: {e}")
        return False

def run_multi_user_crawler(job_id, users_list, get_full_content=False):
    """Ch·∫°y crawler cho multiple users"""
    try:
        # Update job status
        crawl_jobs[job_id]['status'] = 'running'
        crawl_jobs[job_id]['message'] = f'ƒêang crawl {len(users_list)} users...'
        crawl_jobs[job_id]['total_users'] = len(users_list)
        crawl_jobs[job_id]['completed_users'] = 0
        crawl_jobs[job_id]['failed_users'] = 0
        crawl_jobs[job_id]['user_results'] = []
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        all_output_files = []
        
        # Crawl t·ª´ng user
        for i, username_with_id in enumerate(users_list, 1):
            try:
                crawl_jobs[job_id]['message'] = f'Crawl user {i}/{len(users_list)}: {username_with_id}'
                
                # T·∫°o output filename cho user n√†y
                safe_username = username_with_id.replace('.', '_')
                output_filename = f"f319_crawler_{job_id}_{safe_username}_{timestamp}.csv"
                output_path = os.path.join(DOWNLOADS_FOLDER, output_filename)
                
                # Ch·∫°y crawler cho user n√†y
                success = run_single_user_crawler(username_with_id, output_path, get_full_content)
                
                if success:
                    # T√¨m file th·ª±c t·∫ø ƒë∆∞·ª£c t·∫°o (enhanced crawler c√≥ th·ªÉ th√™m _enhanced suffix)
                    pattern_prefix = f"f319_crawler_{job_id}_{safe_username}_{timestamp}"
                    actual_files = []
                    
                    for filename in os.listdir(DOWNLOADS_FOLDER):
                        if filename.startswith(pattern_prefix) and filename.endswith('.csv'):
                            actual_files.append(filename)
                    
                    if actual_files:
                        actual_filename = max(actual_files, key=lambda f: os.path.getctime(os.path.join(DOWNLOADS_FOLDER, f)))
                        actual_path = os.path.join(DOWNLOADS_FOLDER, actual_filename)
                        
                        # ƒê·∫øm s·ªë posts
                        with open(actual_path, 'r', encoding='utf-8') as f:
                            post_count = len(f.readlines()) - 1  # Tr·ª´ header
                        
                        crawl_jobs[job_id]['completed_users'] += 1
                        crawl_jobs[job_id]['user_results'].append({
                            'username': username_with_id,
                            'status': 'success',
                            'filename': actual_filename,
                            'post_count': post_count
                        })
                        all_output_files.append(actual_filename)
                        
                        print(f"‚úÖ User {username_with_id}: {post_count} posts")
                    else:
                        crawl_jobs[job_id]['failed_users'] += 1
                        crawl_jobs[job_id]['user_results'].append({
                            'username': username_with_id,
                            'status': 'error',
                            'message': 'Kh√¥ng t√¨m th·∫•y file output'
                        })
                else:
                    crawl_jobs[job_id]['failed_users'] += 1
                    crawl_jobs[job_id]['user_results'].append({
                        'username': username_with_id,
                        'status': 'error',
                        'message': 'Crawler th·∫•t b·∫°i'
                    })
                
                # Delay gi·ªØa c√°c users
                time.sleep(2)
                
            except Exception as e:
                crawl_jobs[job_id]['failed_users'] += 1
                crawl_jobs[job_id]['user_results'].append({
                    'username': username_with_id,
                    'status': 'error',
                    'message': str(e)
                })
                print(f"‚ùå L·ªói crawl user {username_with_id}: {e}")
        
        # T·∫°o combined file n·∫øu c√≥ nhi·ªÅu h∆°n 1 file th√†nh c√¥ng
        if len(all_output_files) > 1:
            combined_filename = f"f319_crawler_{job_id}_combined_{timestamp}.csv"
            combined_path = os.path.join(DOWNLOADS_FOLDER, combined_filename)
            
            try:
                import csv
                with open(combined_path, 'w', newline='', encoding='utf-8-sig') as combined_file:
                    writer = None
                    total_posts = 0
                    
                    for filename in all_output_files:
                        file_path = os.path.join(DOWNLOADS_FOLDER, filename)
                        if os.path.exists(file_path):
                            with open(file_path, 'r', encoding='utf-8') as f:
                                reader = csv.DictReader(f)
                                
                                if writer is None:
                                    writer = csv.DictWriter(combined_file, fieldnames=reader.fieldnames)
                                    writer.writeheader()
                                
                                for row in reader:
                                    writer.writerow(row)
                                    total_posts += 1
                
                crawl_jobs[job_id]['combined_file'] = combined_filename
                crawl_jobs[job_id]['total_posts'] = total_posts
                print(f"‚úÖ ƒê√£ t·∫°o combined file: {combined_filename} v·ªõi {total_posts} posts")
                
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi t·∫°o combined file: {e}")
        
        # Update final status
        if crawl_jobs[job_id]['completed_users'] > 0:
            crawl_jobs[job_id]['status'] = 'completed'
            crawl_jobs[job_id]['message'] = f'Ho√†n th√†nh! {crawl_jobs[job_id]["completed_users"]}/{len(users_list)} users th√†nh c√¥ng'
            crawl_jobs[job_id]['output_files'] = all_output_files
        else:
            crawl_jobs[job_id]['status'] = 'error'
            crawl_jobs[job_id]['message'] = 'T·∫•t c·∫£ users ƒë·ªÅu th·∫•t b·∫°i'
            
    except Exception as e:
        crawl_jobs[job_id]['status'] = 'error'
        crawl_jobs[job_id]['message'] = f'L·ªói: {str(e)}'
        print(f"‚ùå L·ªói trong multi-user crawler job {job_id}: {e}")

def run_crawler(job_id, username_with_id, get_full_content=False):
    """Ch·∫°y crawler trong background thread (backward compatibility)"""
    try:
        # Parse input ƒë·ªÉ ki·ªÉm tra single hay multi user
        users_list = parse_multiple_users(username_with_id)
        
        if len(users_list) == 1:
            # Single user - d√πng logic c≈©
            crawl_jobs[job_id]['status'] = 'running'
            crawl_jobs[job_id]['message'] = 'ƒêang kh·ªüi t·∫°o crawler...'
            
            # Construct target URL
            target_url = f"https://f319.com/members/{users_list[0]}/"
            
            # Backup original config
            original_url = config.TARGET_URL
            original_output = config.OUTPUT_FILE
            original_headless = config.HEADLESS_MODE
            
            # Set config cho job n√†y
            config.TARGET_URL = target_url
            config.HEADLESS_MODE = True  # Lu√¥n ch·∫°y headless tr√™n web
            
            # T·∫°o unique output filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_filename = f"f319_crawler_{job_id}_{timestamp}.csv"
            output_path = os.path.join(DOWNLOADS_FOLDER, output_filename)
            config.OUTPUT_FILE = output_path
            
            crawl_jobs[job_id]['message'] = 'ƒêang crawl d·ªØ li·ªáu...'
            
            # Ch·∫°y crawler
            crawler = EnhancedF319Crawler()
            success = crawler.run(get_full_content=get_full_content)
            
            # FIX: T√¨m file th·ª±c t·∫ø ƒë∆∞·ª£c t·∫°o (enhanced crawler c√≥ th·ªÉ th√™m _enhanced suffix)
            if success:
                # T√¨m t·∫•t c·∫£ files c√≥ pattern c·ªßa job n√†y
                pattern_prefix = f"f319_crawler_{job_id}_{timestamp}"
                actual_files = []
                
                for filename in os.listdir(DOWNLOADS_FOLDER):
                    if filename.startswith(pattern_prefix) and filename.endswith('.csv'):
                        actual_files.append(filename)
                
                if actual_files:
                    # L·∫•y file m·ªõi nh·∫•t (n·∫øu c√≥ nhi·ªÅu files)
                    actual_filename = max(actual_files, key=lambda f: os.path.getctime(os.path.join(DOWNLOADS_FOLDER, f)))
                    actual_path = os.path.join(DOWNLOADS_FOLDER, actual_filename)
                    
                    # ƒê·∫øm s·ªë posts
                    with open(actual_path, 'r', encoding='utf-8') as f:
                        post_count = len(f.readlines()) - 1  # Tr·ª´ header
                    
                    crawl_jobs[job_id]['status'] = 'completed'
                    crawl_jobs[job_id]['message'] = f'Ho√†n th√†nh! Crawl ƒë∆∞·ª£c {post_count} posts'
                    crawl_jobs[job_id]['output_file'] = actual_filename  # D√πng t√™n file th·ª±c t·∫ø
                    crawl_jobs[job_id]['post_count'] = post_count
                else:
                    crawl_jobs[job_id]['status'] = 'error'
                    crawl_jobs[job_id]['message'] = 'Crawler ho√†n th√†nh nh∆∞ng kh√¥ng t√¨m th·∫•y file output'
            else:
                crawl_jobs[job_id]['status'] = 'error'
                crawl_jobs[job_id]['message'] = 'Crawler g·∫∑p l·ªói ho·∫∑c kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu'
            
            # Restore original config
            config.TARGET_URL = original_url
            config.OUTPUT_FILE = original_output
            config.HEADLESS_MODE = original_headless
            
        else:
            # Multiple users - d√πng logic m·ªõi
            run_multi_user_crawler(job_id, users_list, get_full_content)
        
    except Exception as e:
        crawl_jobs[job_id]['status'] = 'error'
        crawl_jobs[job_id]['message'] = f'L·ªói: {str(e)}'
        print(f"‚ùå L·ªói trong crawler job {job_id}: {e}")

@app.route('/')
def index():
    """Trang ch√≠nh"""
    cleanup_old_files()
    return render_template('index.html')

@app.route('/crawl', methods=['POST'])
def start_crawl():
    """B·∫Øt ƒë·∫ßu crawl job"""
    try:
        # L·∫•y input t·ª´ form
        username_with_id = request.form.get('username_id', '').strip()
        get_full_content = request.form.get('full_content') == 'on'
        max_full_content = request.form.get('max_full_content', '0').strip()
        
        # Validate input
        if not username_with_id:
            flash('Vui l√≤ng nh·∫≠p username.userid', 'error')
            return redirect(url_for('index'))
        
        # Validate max_full_content
        try:
            max_full_content_int = int(max_full_content) if max_full_content else 0
            if max_full_content_int < -1:
                max_full_content_int = 0
        except ValueError:
            max_full_content_int = 0
        
        # C·∫≠p nh·∫≠t config v·ªõi t√πy ch·ªçn t·ª´ user
        if get_full_content:
            config.MAX_FULL_CONTENT_POSTS = max_full_content_int
            if max_full_content_int == 0:
                flash('S·∫Ω l·∫•y full content cho T·∫§T C·∫¢ posts (c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian)', 'warning')
            elif max_full_content_int == -1:
                flash('S·∫Ω ch·ªâ l·∫•y snippet content (nhanh)', 'info')
                get_full_content = False
            else:
                flash(f'S·∫Ω l·∫•y full content cho {max_full_content_int} posts ƒë·∫ßu ti√™n', 'info')
        
        # Parse multiple users
        users_list = parse_multiple_users(username_with_id)
        
        if not users_list:
            flash('Kh√¥ng t√¨m th·∫•y user n√†o h·ª£p l·ªá. Format ph·∫£i l√† username.userid', 'error')
            return redirect(url_for('index'))
        
        # Validate format cho t·ª´ng user
        for user in users_list:
            if '.' not in user:
                flash(f'Format kh√¥ng h·ª£p l·ªá: {user}. Ph·∫£i l√† username.userid', 'error')
                return redirect(url_for('index'))
        
        # T·∫°o job ID
        job_id = str(uuid.uuid4())[:8]
        
        # Initialize job tracking
        crawl_jobs[job_id] = {
            'status': 'starting',
            'message': 'ƒêang chu·∫©n b·ªã...',
            'username_id': username_with_id,
            'users_list': users_list,
            'full_content': get_full_content,
            'max_full_content': max_full_content_int,
            'start_time': datetime.now(),
            'output_file': None,
            'post_count': 0
        }
        
        # Start crawler trong background thread
        thread = threading.Thread(target=run_crawler, args=(job_id, username_with_id, get_full_content))
        thread.daemon = True
        thread.start()
        
        if len(users_list) == 1:
            flash(f'ƒê√£ b·∫Øt ƒë·∫ßu crawl cho {users_list[0]}', 'success')
        else:
            flash(f'ƒê√£ b·∫Øt ƒë·∫ßu crawl cho {len(users_list)} users', 'success')
        
        return redirect(url_for('result', job_id=job_id))
        
    except Exception as e:
        flash(f'L·ªói khi b·∫Øt ƒë·∫ßu crawl: {str(e)}', 'error')
        return redirect(url_for('index'))

@app.route('/result/<job_id>')
def result(job_id):
    """Trang k·∫øt qu·∫£"""
    if job_id not in crawl_jobs:
        flash('Job kh√¥ng t·ªìn t·∫°i', 'error')
        return redirect(url_for('index'))
    
    job_data = crawl_jobs[job_id]
    return render_template('result.html', job_id=job_id, job_data=job_data)

@app.route('/status/<job_id>')
def job_status(job_id):
    """API endpoint ƒë·ªÉ check status c·ªßa job"""
    if job_id not in crawl_jobs:
        return jsonify({'status': 'not_found', 'message': 'Job kh√¥ng t·ªìn t·∫°i'})
    
    job_data = crawl_jobs[job_id].copy()
    
    # Calculate elapsed time
    if 'start_time' in job_data:
        elapsed = datetime.now() - job_data['start_time']
        job_data['elapsed_time'] = str(elapsed).split('.')[0]  # Remove microseconds
    
    return jsonify(job_data)

@app.route('/download/<job_id>')
def download_file(job_id):
    """Download file k·∫øt qu·∫£"""
    try:
        if job_id not in crawl_jobs:
            flash('Job kh√¥ng t·ªìn t·∫°i', 'error')
            return redirect(url_for('index'))
        
        job_data = crawl_jobs[job_id]
        
        if job_data['status'] != 'completed':
            flash('File ch∆∞a s·∫µn s√†ng ƒë·ªÉ download', 'error')
            return redirect(url_for('result', job_id=job_id))
        
        # ∆Øu ti√™n combined file n·∫øu c√≥
        if job_data.get('combined_file'):
            file_path = os.path.join(DOWNLOADS_FOLDER, job_data['combined_file'])
            download_name = job_data['combined_file']
        elif job_data.get('output_file'):
            file_path = os.path.join(DOWNLOADS_FOLDER, job_data['output_file'])
            download_name = job_data['output_file']
        else:
            flash('Kh√¥ng t√¨m th·∫•y file ƒë·ªÉ download', 'error')
            return redirect(url_for('result', job_id=job_id))
        
        if not os.path.exists(file_path):
            flash('File kh√¥ng t·ªìn t·∫°i', 'error')
            return redirect(url_for('result', job_id=job_id))
        
        return send_file(file_path, as_attachment=True, download_name=download_name)
        
    except Exception as e:
        flash(f'L·ªói khi download: {str(e)}', 'error')
        return redirect(url_for('result', job_id=job_id))

@app.route('/download_individual/<job_id>/<filename>')
def download_individual_file(job_id, filename):
    """Download individual file t·ª´ multi-user crawl"""
    try:
        if job_id not in crawl_jobs:
            flash('Job kh√¥ng t·ªìn t·∫°i', 'error')
            return redirect(url_for('index'))
        
        file_path = os.path.join(DOWNLOADS_FOLDER, filename)
        
        if not os.path.exists(file_path):
            flash('File kh√¥ng t·ªìn t·∫°i', 'error')
            return redirect(url_for('result', job_id=job_id))
        
        return send_file(file_path, as_attachment=True, download_name=filename)
        
    except Exception as e:
        flash(f'L·ªói khi download: {str(e)}', 'error')
        return redirect(url_for('result', job_id=job_id))

@app.route('/jobs')
def list_jobs():
    """Danh s√°ch t·∫•t c·∫£ jobs (admin)"""
    return jsonify(crawl_jobs)

if __name__ == '__main__':
    print("üöÄ === F319 CRAWLER WEB INTERFACE (Enhanced Multi-User) ===")
    print("üåê Truy c·∫≠p: http://localhost:5000")
    print("üìã Nh·∫≠p username.userid ƒë·ªÉ b·∫Øt ƒë·∫ßu crawl")
    print("üî¢ H·ªó tr·ª£ multiple users (c√°ch nhau b·∫±ng d√≤ng m·ªõi ho·∫∑c d·∫•u ph·∫©y)")
    print("=" * 50)
    
    app.run(debug=True, host='0.0.0.0', port=5000) 